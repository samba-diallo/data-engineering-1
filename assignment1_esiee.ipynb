{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fccb597",
   "metadata": {},
   "source": [
    "# ESIEE Paris — Data Engineering I — Assignment 1\n",
    "> Author : Badr TAJINI\n",
    "\n",
    "**Academic year:** 2025–2026  \n",
    "**Program:** Data & Applications - Engineering - (FD)   \n",
    "**Course:** Data Engineering I  \n",
    "\n",
    "---\n",
    "\n",
    "In this assignment, you'll make sure that you've correctly set up your local Spark environment.\n",
    "You'll then complete a classic \"Word Count\" task on the `description` column of the `a1-brand.csv` file.\n",
    "\n",
    "You can think of \"Word Count\" as the \"Hello World!\" of Hadoop, Spark, etc.\n",
    "The task is simple: We want to count the total number of times each word occurs (in a potentially large collection of text).\n",
    "Typically, we want to sort by the counts in descending order so we can examine the most frequently occurring words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac1598",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- Confirm local Spark environment in JupyterLab.\n",
    "- Implement word-count using **RDD** and **DataFrame** APIs.\n",
    "- Produce top-10 tokens with and without stopwords.\n",
    "- Record brief performance notes and environment details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853064b",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "The following code snippet should \"just work\" to initialize Spark.\n",
    "If it doesn't, consult the **helper and Lab 0 with installation and setup guide**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d1601-4445-4fa7-8ca9-656dd87679c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark, os\n",
    "#os.environ[\"SPARK_HOME\"] = \"/path/to/spark-4.0.0-bin-hadoop3\"\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba0fce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b32434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: /home/sable/miniconda3/envs/de1-env\n",
      "SPARK_HOME: /home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "# Configurer JAVA_HOME\n",
    "os.environ['JAVA_HOME'] = '/home/sable/miniconda3/envs/de1-env'\n",
    "\n",
    "# Configurer SPARK_HOME correctement\n",
    "os.environ['SPARK_HOME'] = '/home/sable/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark'\n",
    "\n",
    "# Vérifier les configurations\n",
    "print(f\"JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "print(f\"SPARK_HOME: {os.environ['SPARK_HOME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17950740",
   "metadata": {},
   "source": [
    "Edit the path below to point to your local copy of `a1-brand.csv`. \n",
    "\n",
    "Examples:\n",
    "- macOS/Linux: `/Users/yourname/data/a1-brand.csv`\n",
    "- Windows: `C:\\\\Users\\\\yourname\\\\data\\\\a1-brand.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "578fd221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the path to a1-brand.csv\n",
    "DATA_PATH = \"/path/to/a1-brand.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdc36a",
   "metadata": {},
   "source": [
    "Import PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "636b041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaebd90",
   "metadata": {},
   "source": [
    "Set up to measure wall time and memory. (Don't worry about the details, just run the cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "980f90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import time, os, platform\n",
    "import psutil, resource\n",
    "\n",
    "def _rss_bytes():\n",
    "    return psutil.Process(os.getpid()).memory_info().rss\n",
    "\n",
    "def _ru_maxrss_bytes():\n",
    "    # ru_maxrss: bytes on macOS; kilobytes on Linux\n",
    "    ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    if platform.system() == \"Darwin\":\n",
    "        return int(ru)  # bytes\n",
    "    else:\n",
    "        return int(ru) * 1024  # KB -> bytes\n",
    "\n",
    "@register_cell_magic\n",
    "def timemem(line, cell):\n",
    "    \"\"\"\n",
    "    Measure wall time and memory around the execution of this cell.\n",
    "    Usage:\n",
    "        %%timemem\n",
    "        <your code>\n",
    "    \"\"\"\n",
    "    ip = get_ipython()\n",
    "    rss_before = _rss_bytes()\n",
    "    peak_before = _ru_maxrss_bytes()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Execute the cell body\n",
    "    result = ip.run_cell(cell)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    rss_after = _rss_bytes()\n",
    "    peak_after = _ru_maxrss_bytes()\n",
    "\n",
    "    wall = t1 - t0\n",
    "    rss_delta_mb = (rss_after - rss_before) / (1024*1024)\n",
    "    peak_delta_mb = (peak_after - peak_before) / (1024*1024)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Wall time: {wall:.3f} s\")\n",
    "    print(f\"RSS Δ: {rss_delta_mb:+.2f} MB\")\n",
    "    print(f\"Peak memory Δ: {peak_delta_mb:+.2f} MB (OS-dependent)\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf067",
   "metadata": {},
   "source": [
    "Start a local Spark session (i.e., a `SparkContext`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3506b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/22 00:59:51 WARN Utils: Your hostname, sable-ThinkPad-X1-Yoga-3rd, resolves to a loopback address: 127.0.1.1; using 10.192.33.105 instead (on interface wlp2s0)\n",
      "25/10/22 00:59:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/22 00:59:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession créée avec succès!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.192.33.105:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Assignment1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x78dd3908d510>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer la SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Assignment1\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession créée avec succès!\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2619ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "150dde24",
   "metadata": {},
   "source": [
    "If you've gotten to here, congrats! Everything seems to have been set up and initialized properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60c051-264c-4c9d-88ef-d9a24c241d8e",
   "metadata": {},
   "source": [
    "## 2. Word Count with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d740f5-5dfd-42b0-905e-3fd3a7f20134",
   "metadata": {},
   "source": [
    "First, let's read the `a1-brand.csv` file into an RDD.\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You'll want to fetch the `SparkContext` from the `SparkSession`.\n",
    "- There's a method of the `SparkContext` for reading in text files.\n",
    "- This simple exercise should only take two lines. If you find yourself writing more code, you're doing something wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42c7ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7262"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 3.554 s\n",
      "RSS Δ: +0.12 MB\n",
      "Peak memory Δ: +0.12 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 78dd39109c30, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 78dd39109a80, raw_cell=\"\n",
       "# TODO: Write your code below, but do not remove ..\" store_history=False silent=False shell_futures=True cell_id=None> result=7262>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"a1-brand.csv\")\n",
    "\n",
    "# By the time we get to here, \"lines\" should refer to an RDD with the brand file loaded.\n",
    "# Let's count the lines.\n",
    "\n",
    "\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb387ef",
   "metadata": {},
   "source": [
    "Next, clean and tokenize text, and then find the 10 most common words.\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Required Steps:**\n",
    "\n",
    "- Lowercase all text.\n",
    "- Replace non-letter characters (`[^a-z]`) with spaces.\n",
    "- Split on whitespace into tokens.\n",
    "- Remove tokens with length < 2.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You _must_ use `flatMap` and other RDD operations in this step. If you're not, you're doing something wrong...\n",
    "- At the end, you'll need to `collect` the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "806a5885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and: 16150\n",
      "the: 9612\n",
      "in: 7958\n",
      "is: 7814\n",
      "for: 6789\n",
      "brand: 6476\n",
      "its: 4241\n",
      "to: 4026\n",
      "of: 3382\n",
      "with: 3099\n",
      "======================================\n",
      "Wall time: 2.717 s\n",
      "RSS Δ: +0.38 MB\n",
      "Peak memory Δ: +0.38 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 78dd392f0370, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 78dd392f0dc0, raw_cell=\"\n",
       "# TODO: Write your code below, but do not remove ..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "import re\n",
    "\n",
    "# Clean and tokenize:\n",
    "# - lowercase\n",
    "# - replace non-letters with spaces\n",
    "# - split on whitespace\n",
    "# - drop tokens of length < 2\n",
    "words = (\n",
    "    lines\n",
    "    .map(lambda s: re.sub('[^a-z]', ' ', s.lower()))\n",
    "    .flatMap(lambda s: s.split())\n",
    "    .filter(lambda w: len(w) >= 2)\n",
    ")\n",
    "\n",
    "# Count and get the 10 most frequent words\n",
    "word_counts = (\n",
    "    words\n",
    "    .map(lambda w: (w, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .sortBy(lambda kv: (-kv[1], kv[0]))\n",
    "    .collect()  # Collecter TOUT en une liste Python\n",
    ")\n",
    "\n",
    "# By the time we get to here \"word_counts\" already has the collected output, sorted by frequency in descending order.\n",
    "# So we just print out the top-10.\n",
    "\n",
    "for word, count in word_counts[:10]:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d215c",
   "metadata": {},
   "source": [
    "## 3. Word Count with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5672f0-63ec-4f0e-b784-1684e1c7a7e8",
   "metadata": {},
   "source": [
    "### 3.1 Again, Just with DataFrames\n",
    "\n",
    "Now, we're going to do the same thing, but with DataFrames instead of RDDs.\n",
    "\n",
    "What's the difference, you ask? We'll cover it in lecture soon enough!\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Here, you'll use the `SparkSession`.\n",
    "- Loading a DataFrame is a single method call. If you find yourself writing more code, you're doing something wrong...\n",
    "- When loading the CSV file, be aware of your escape character; use something like `.option(\"escape\", ...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e99b288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 7261\n",
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------+\n",
      "|                                                                     description|\n",
      "+--------------------------------------------------------------------------------+\n",
      "|a-case is a brand specializing in protective accessories for electronic devic...|\n",
      "|A-Derma is a French dermatological skincare brand specializing in products fo...|\n",
      "| a patented ingredient derived from oat plants cultivated under organic farmi...|\n",
      "|                                                                       cleansers|\n",
      "|           A-Derma emphasizes clinical efficacy and hypoallergenic formulations.|\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "======================================\n",
      "Wall time: 0.689 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 78dd392f0e50, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 78dd392f2950, raw_cell=\"\n",
       "# TODO: Write your code below, but do not remove ..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# Charger le fichier CSV dans un DataFrame\n",
    "df = (spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")           # Le fichier a une ligne d'en-tête\n",
    "      .option(\"escape\", \"\\\"\")             # Utiliser guillemet double comme caractère d'échappement\n",
    "      .option(\"inferSchema\", \"true\")      # Inférer automatiquement les types de colonnes\n",
    "      .csv(\"a1-brand.csv\")\n",
    "     )\n",
    "\n",
    "# By the time we get to here, the file should have already been loaded into a DataFrame.\n",
    "# Here, we just inspect it.\n",
    "\n",
    "print(\"Rows:\", df.count())\n",
    "df.printSchema()\n",
    "df.select(\"description\").show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882eacc",
   "metadata": {},
   "source": [
    "Next, clean and tokenize text, and then find the 10 most common (i.e., frequently occurring) words.\n",
    "This attempts the same processing as word count with RDDs above, except here you're using a DataFrame.\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Required Steps:** (Exactly the same as above.)\n",
    "\n",
    "- Lowercase all text.\n",
    "- Replace non-letter characters (`[^a-z]`) with spaces.\n",
    "- Split on whitespace into tokens.\n",
    "- Remove tokens with length < 2.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You _must_ use `explode` and other Spark DataFrame operations in this exercise.\n",
    "- This exercise shouldn't take more than (roughly) a dozen lines. If you find yourself writing more code, you're doing something wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dec84df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  and|13094|\n",
      "|  the| 6895|\n",
      "|   is| 6419|\n",
      "|   in| 6351|\n",
      "|  for| 5530|\n",
      "|brand| 5196|\n",
      "|  its| 3304|\n",
      "|   to| 3155|\n",
      "|   of| 2692|\n",
      "|known| 2509|\n",
      "+-----+-----+\n",
      "\n",
      "======================================\n",
      "Wall time: 1.172 s\n",
      "RSS Δ: +0.00 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 78dd397a76d0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 78dd2bfb9900, raw_cell=\"\n",
       "# TODO: Write your code below, but do not remove ..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, count\n",
    "\n",
    "# Clean, tokenize, and count words using DataFrame operations\n",
    "word_counts = (\n",
    "    df\n",
    "    .select(\"description\")                                      # Sélectionner la colonne description\n",
    "    .withColumn(\"clean\", lower(col(\"description\")))            # Convertir en minuscules\n",
    "    .withColumn(\"clean\", regexp_replace(col(\"clean\"), \"[^a-z]\", \" \"))  # Remplacer non-lettres par espaces\n",
    "    .withColumn(\"words\", split(col(\"clean\"), \"\\\\s+\"))          # Séparer en tokens sur les espaces\n",
    "    .withColumn(\"word\", explode(col(\"words\")))                 # Exploser le tableau en lignes individuelles\n",
    "    .filter(length(col(\"word\")) >= 2)                          # Filtrer tokens de longueur >= 2\n",
    "    .groupBy(\"word\")                                           # Grouper par mot\n",
    "    .agg(count(\"*\").alias(\"count\"))                            # Compter les occurrences\n",
    "    .orderBy(col(\"count\").desc(), col(\"word\"))                 # Trier par fréquence desc, puis alphabétiquement\n",
    ")\n",
    "\n",
    "\n",
    "# By the time we get to here \"word_counts\" is a DataFrame that already has the word counts sorted in descending order.\n",
    "# So we just print out the top-10.\n",
    "\n",
    "top10 = word_counts.limit(10)\n",
    "top10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede1ca2",
   "metadata": {},
   "source": [
    "**Questions to reflect on**:\n",
    "\n",
    "- What is conceptually different about how Spark executes `flatMap` and `explode`?\n",
    "- What are the advantages or disadvantages of using each of them? \n",
    "- Are there cases where you may prefer one over the other?\n",
    "\n",
    "(No need to write answers in the assignment submission. Just think about it...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505e64e",
   "metadata": {},
   "source": [
    "**Question to actually answer**:\n",
    "\n",
    "Does the RDD approach and the DataFrame approach give the same answers? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a1265-558b-464f-8c85-e9c553ba9fad",
   "metadata": {},
   "source": [
    "**Write your answer to the above question!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79bd6f",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "**Yes and No - It depends on the RDD implementation.**\n",
    "\n",
    "### Why they may differ:\n",
    "\n",
    "**1. CSV Parsing (MAIN DIFFERENCE):**\n",
    "- **RDD**: Reads each line as raw text → processes EVERYTHING (IDs, brand names, descriptions)\n",
    "- **DataFrame**: Parses CSV correctly → processes ONLY the \"description\" column\n",
    "\n",
    "**2. Header handling:**\n",
    "- **RDD**: Likely includes \"id,brand,description\" in the word count\n",
    "- **DataFrame**: Automatically ignores the header with `option(\"header\", \"true\")`\n",
    "\n",
    "**3. Same cleaning logic:**\n",
    "Both apply the same transformations (lowercase, regex, filtering), so **if the RDD also processed only the description column**, the results would be identical.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "**In practice, the results are probably DIFFERENT** because:\n",
    "- RDD treats all columns (unstructured)\n",
    "- DataFrame treats only \"description\" (structured)\n",
    "\n",
    "**The DataFrame gives more accurate results** for analyzing specifically the textual content of the \"description\" column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ca759",
   "metadata": {},
   "source": [
    "### 3.1 Removing Stopwords\n",
    "\n",
    "You've probably noticed that many of the most frequently occurring words are not providing us any indication about the content because they are words like \"in\", \"the\", \"for\", etc.\n",
    "These are called stopwords.\n",
    "\n",
    "Let's remove stopwords and count again!\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Filter out all stopwords from the DataFrame before counting.\n",
    "- Use `StopWordsRemover` from `pyspark.ml.feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "341c7d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|       brand| 5196|\n",
      "|       known| 2509|\n",
      "|    products| 2459|\n",
      "|   primarily| 2100|\n",
      "|      market| 1873|\n",
      "|       range| 1688|\n",
      "|  recognized| 1482|\n",
      "|   including| 1452|\n",
      "|specializing| 1390|\n",
      "|       often| 1247|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "import numpy\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, count, size, array_remove\n",
    "\n",
    "# Créer un StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Clean, tokenize, remove stopwords, and count words\n",
    "word_counts_noStopWords = (\n",
    "    df\n",
    "    .select(\"description\")                                      # Sélectionner la colonne description\n",
    "    .filter(col(\"description\").isNotNull())                    # Supprimer les lignes null\n",
    "    .withColumn(\"clean\", lower(col(\"description\")))            # Convertir en minuscules\n",
    "    .withColumn(\"clean\", regexp_replace(col(\"clean\"), \"[^a-z]\", \" \"))  # Remplacer non-lettres par espaces\n",
    "    .withColumn(\"words\", split(col(\"clean\"), \"\\\\s+\"))          # Séparer en tokens\n",
    "    .withColumn(\"words\", array_remove(col(\"words\"), \"\"))       # Supprimer les strings vides du tableau\n",
    "    .filter(size(col(\"words\")) > 0)                            # Garder seulement les lignes avec des mots\n",
    "    .transform(lambda df: remover.transform(df))               # Supprimer les stopwords\n",
    "    .withColumn(\"word\", explode(col(\"filtered_words\")))        # Exploser le tableau filtré\n",
    "    .filter(length(col(\"word\")) >= 2)                          # Filtrer tokens de longueur >= 2\n",
    "    .groupBy(\"word\")                                           # Grouper par mot\n",
    "    .agg(count(\"*\").alias(\"count\"))                            # Compter les occurrences\n",
    "    .orderBy(col(\"count\").desc(), col(\"word\"))                 # Trier par fréquence desc\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# By the time we get to here \"word_counts_noStopWords\" is a DataFrame that already has the word counts sorted in descending order.\n",
    "# So we just print out the top-10.\n",
    "\n",
    "top10_noStopWords = word_counts_noStopWords.limit(10)\n",
    "top10_noStopWords.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623f34a",
   "metadata": {},
   "source": [
    "### 3.2 Saving Results to CSV\n",
    "\n",
    "+ Save the results of the top-10 most frequently occurring words _with stopwords_, as a CSV file, to `top10_words.csv`.\n",
    "+ Save the results of the top-10 frequently occurring words _discarding stopwords_, as a CSV file, to `top10_noStopWords.csv`.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5e082af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "# Sauvegarder le top 10 avec stopwords\n",
    "top10 = word_counts.limit(10)\n",
    "top10.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"top10_words.csv\")\n",
    "\n",
    "# Sauvegarder le top 10 sans stopwords\n",
    "top10_noStopWords = word_counts_noStopWords.limit(10)\n",
    "top10_noStopWords.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"top10_noStopWords.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "683125e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Top 10 avec stopwords ===\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  and|13094|\n",
      "|  the| 6895|\n",
      "|   is| 6419|\n",
      "|   in| 6351|\n",
      "|  for| 5530|\n",
      "|brand| 5196|\n",
      "|  its| 3304|\n",
      "|   to| 3155|\n",
      "|   of| 2692|\n",
      "|known| 2509|\n",
      "+-----+-----+\n",
      "\n",
      "\n",
      "=== Top 10 sans stopwords ===\n",
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|       brand| 5196|\n",
      "|       known| 2509|\n",
      "|    products| 2459|\n",
      "|   primarily| 2100|\n",
      "|      market| 1873|\n",
      "|       range| 1688|\n",
      "|  recognized| 1482|\n",
      "|   including| 1452|\n",
      "|specializing| 1390|\n",
      "|       often| 1247|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lire et afficher top10_words.csv\n",
    "print(\"=== Top 10 avec stopwords ===\")\n",
    "df_top10 = spark.read.option(\"header\", \"true\").csv(\"top10_words.csv\")\n",
    "df_top10.show()\n",
    "\n",
    "# Lire et afficher top10_noStopWords.csv\n",
    "print(\"\\n=== Top 10 sans stopwords ===\")\n",
    "df_top10_noStopWords = spark.read.option(\"header\", \"true\").csv(\"top10_noStopWords.csv\")\n",
    "df_top10_noStopWords.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89fac7f",
   "metadata": {},
   "source": [
    "## 4. Assignment Submission and Cleanup\n",
    "\n",
    "Details about the Submission of this assignment are outlined in the helper. Please read carefully the instructions.\n",
    "\n",
    "Finally, clean up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a41b8a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vérification des fichiers de sortie...\n",
      "top10_words.csv existe: True\n",
      "top10_noStopWords.csv existe: True\n",
      "\n",
      "Contenu de top10_words.csv/:\n",
      "total 4,0K\n",
      "-rw-r--r-- 1 sable sable 102 oct.  22 02:00 part-00000-dee28def-7b7d-4df2-9f83-c1c5ded32998-c000.csv\n",
      "-rw-r--r-- 1 sable sable   0 oct.  22 01:57 _SUCCESS\n",
      "\n",
      "Contenu de top10_noStopWords.csv/:\n",
      "total 4,0K\n",
      "-rw-r--r-- 1 sable sable 145 oct.  22 01:57 part-00000-2608e003-ab9e-402b-92a6-b97acdf255c4-c000.csv\n",
      "-rw-r--r-- 1 sable sable   0 oct.  22 01:57 _SUCCESS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dans une cellule de votre notebook, vérifiez que tous les fichiers sont créés\n",
    "import os\n",
    "\n",
    "print(\"✓ Vérification des fichiers de sortie...\")\n",
    "print(f\"top10_words.csv existe: {os.path.exists('top10_words.csv')}\")\n",
    "print(f\"top10_noStopWords.csv existe: {os.path.exists('top10_noStopWords.csv')}\")\n",
    "\n",
    "# Lister les fichiers dans ces dossiers\n",
    "print(\"\\nContenu de top10_words.csv/:\")\n",
    "os.system(\"ls -lh top10_words.csv/\")\n",
    "\n",
    "print(\"\\nContenu de top10_noStopWords.csv/:\")\n",
    "os.system(\"ls -lh top10_noStopWords.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b071ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SparkSession arrêtée avec succès!\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"✓ SparkSession arrêtée avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96409bb6",
   "metadata": {},
   "source": [
    "## Performance notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c5299",
   "metadata": {},
   "source": [
    "- Prefer DataFrame built-ins; avoid Python UDFs for tokenization where possible.\n",
    "- Keep shuffle partitions modest on local runs.\n",
    "- Cache wisely and avoid unnecessary actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f18f3",
   "metadata": {},
   "source": [
    "## Reproducibility checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64031975",
   "metadata": {},
   "source": [
    "- Record Python/Java/Spark versions.\n",
    "- Fix timezone to UTC.\n",
    "- Provide exact run command and paths to input/output files.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
