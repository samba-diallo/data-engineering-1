{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fccb597",
   "metadata": {},
   "source": [
    "# ESIEE Paris — Data Engineering I — Assignment 1\n",
    "> Author : Badr TAJINI\n",
    "\n",
    "**Academic year:** 2025–2026  \n",
    "**Program:** Data & Applications - Engineering - (FD)   \n",
    "**Course:** Data Engineering I  \n",
    "\n",
    "---\n",
    "\n",
    "In this assignment, you'll make sure that you've correctly set up your local Spark environment.\n",
    "You'll then complete a classic \"Word Count\" task on the `description` column of the `a1-brand.csv` file.\n",
    "\n",
    "You can think of \"Word Count\" as the \"Hello World!\" of Hadoop, Spark, etc.\n",
    "The task is simple: We want to count the total number of times each word occurs (in a potentially large collection of text).\n",
    "Typically, we want to sort by the counts in descending order so we can examine the most frequently occurring words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac1598",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- Confirm local Spark environment in JupyterLab.\n",
    "- Implement word-count using **RDD** and **DataFrame** APIs.\n",
    "- Produce top-10 tokens with and without stopwords.\n",
    "- Record brief performance notes and environment details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853064b",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "The following code snippet should \"just work\" to initialize Spark.\n",
    "If it doesn't, consult the **helper and Lab 0 with installation and setup guide**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10d1601-4445-4fa7-8ca9-656dd87679c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, os\n",
    "#os.environ[\"SPARK_HOME\"] = \"/path/to/spark-4.0.1-bin-hadoop3\"\n",
    "import findspark, os\n",
    "# Supprimez ou commentez cette ligne :\n",
    "# os.environ[\"SPARK_HOME\"] = \"/path/to/spark-4.0.0-bin-hadoop3\"\n",
    "\n",
    "# Initialisez findspark sans SPARK_HOME défini\n",
    "# (il utilisera le pyspark installé via conda/pip)\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f5a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import findspark, os\n",
    "# remove or fix the fake SPARK_HOME line\n",
    "os.environ[\"SPARK_HOME\"] = \"/path/to/spark-4.0.1-bin-hadoop3\"\n",
    "findspark.init()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17950740",
   "metadata": {},
   "source": [
    "Edit the path below to point to your local copy of `a1-brand.csv`. \n",
    "\n",
    "Examples:\n",
    "- macOS/Linux: `/Users/yourname/data/a1-brand.csv`\n",
    "- Windows: `C:\\\\Users\\\\yourname\\\\data\\\\a1-brand.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "578fd221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the path to a1-brand.csv\n",
    "DATA_PATH = \"/path/to/a1-brand.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdc36a",
   "metadata": {},
   "source": [
    "Import PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "636b041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaebd90",
   "metadata": {},
   "source": [
    "Set up to measure wall time and memory. (Don't worry about the details, just run the cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "980f90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import time, os, platform\n",
    "import psutil, resource\n",
    "\n",
    "def _rss_bytes():\n",
    "    return psutil.Process(os.getpid()).memory_info().rss\n",
    "\n",
    "def _ru_maxrss_bytes():\n",
    "    # ru_maxrss: bytes on macOS; kilobytes on Linux\n",
    "    ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    if platform.system() == \"Darwin\":\n",
    "        return int(ru)  # bytes\n",
    "    else:\n",
    "        return int(ru) * 1024  # KB -> bytes\n",
    "\n",
    "@register_cell_magic\n",
    "def timemem(line, cell):\n",
    "    \"\"\"\n",
    "    Measure wall time and memory around the execution of this cell.\n",
    "    Usage:\n",
    "        %%timemem\n",
    "        <your code>\n",
    "    \"\"\"\n",
    "    ip = get_ipython()\n",
    "    rss_before = _rss_bytes()\n",
    "    peak_before = _ru_maxrss_bytes()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Execute the cell body\n",
    "    result = ip.run_cell(cell)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    rss_after = _rss_bytes()\n",
    "    peak_after = _ru_maxrss_bytes()\n",
    "\n",
    "    wall = t1 - t0\n",
    "    rss_delta_mb = (rss_after - rss_before) / (1024*1024)\n",
    "    peak_delta_mb = (peak_after - peak_before) / (1024*1024)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Wall time: {wall:.3f} s\")\n",
    "    print(f\"RSS Δ: {rss_delta_mb:+.2f} MB\")\n",
    "    print(f\"Peak memory Δ: {peak_delta_mb:+.2f} MB (OS-dependent)\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf067",
   "metadata": {},
   "source": [
    "Start a local Spark session (i.e., a `SparkContext`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3506b62",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/spark-4.0.1-bin-hadoop3/./bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAssignment1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Use all local cores\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.ui.showConsoleProgress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m spark\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/sql/session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/core/context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/core/context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/site-packages/pyspark/java_gateway.py:101\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, signal\u001b[38;5;241m.\u001b[39mSIG_IGN)\n\u001b[1;32m    100\u001b[0m     popen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreexec_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m preexec_func\n\u001b[0;32m--> 101\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m~/miniconda3/envs/de1-env/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/spark-4.0.1-bin-hadoop3/./bin/spark-submit'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 0.435 s\n",
      "RSS Δ: +0.12 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 78dd392c2ec0, execution_count=None error_before_exec=None error_in_exec=[Errno 2] No such file or directory: '/path/to/spark-4.0.1-bin-hadoop3/./bin/spark-submit' info=<ExecutionInfo object at 78dd39742bc0, raw_cell=\"\n",
       "spark = (\n",
       "    SparkSession.builder\n",
       "    .appName(\"..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Assignment1\")\n",
    "    .master(\"local[*]\")            # Use all local cores\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150dde24",
   "metadata": {},
   "source": [
    "If you've gotten to here, congrats! Everything seems to have been set up and initialized properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60c051-264c-4c9d-88ef-d9a24c241d8e",
   "metadata": {},
   "source": [
    "## 2. Word Count with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d740f5-5dfd-42b0-905e-3fd3a7f20134",
   "metadata": {},
   "source": [
    "First, let's read the `a1-brand.csv` file into an RDD.\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You'll want to fetch the `SparkContext` from the `SparkSession`.\n",
    "- There's a method of the `SparkContext` for reading in text files.\n",
    "- This simple exercise should only take two lines. If you find yourself writing more code, you're doing something wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c7ef3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Write your code below, but do not remove any lines already in this cell.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[1;32m      4\u001b[0m lines \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma1-brand.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# By the time we get to here, \"lines\" should refer to an RDD with the brand file loaded.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Let's count the lines.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Wall time: 0.054 s\n",
      "RSS Δ: +0.12 MB\n",
      "Peak memory Δ: +0.00 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 78dd392c2ef0, execution_count=None error_before_exec=None error_in_exec=name 'spark' is not defined info=<ExecutionInfo object at 78dd392c2aa0, raw_cell=\"\n",
       "# TODO: Write your code below, but do not remove ..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"a1-brand.csv\")\n",
    "\n",
    "# By the time we get to here, \"lines\" should refer to an RDD with the brand file loaded.\n",
    "# Let's count the lines.\n",
    "\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb387ef",
   "metadata": {},
   "source": [
    "Next, clean and tokenize text, and then find the 10 most common words.\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Required Steps:**\n",
    "\n",
    "- Lowercase all text.\n",
    "- Replace non-letter characters (`[^a-z]`) with spaces.\n",
    "- Split on whitespace into tokens.\n",
    "- Remove tokens with length < 2.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You _must_ use `flatMap` and other RDD operations in this step. If you're not, you're doing something wrong...\n",
    "- At the end, you'll need to `collect` the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "806a5885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Clean and tokenize text:\n",
    "# 1. Convert all text to lowercase\n",
    "# 2. Replace non-letter characters with spaces\n",
    "# 3. Split on whitespace to get tokens\n",
    "# 4. Filter out tokens with length < 2\n",
    "# 5. Count occurrences of each word\n",
    "# 6. Sort by frequency (descending) and alphabetically for ties\n",
    "# 7. Collect results to driver\n",
    "\n",
    "word_counts = (\n",
    "    lines\n",
    "    .map(lambda s: re.sub('[^a-z]', ' ', s.lower()))  # lowercase and replace non-letters with spaces\n",
    "    .flatMap(lambda s: s.split())                     # split on whitespace into tokens\n",
    "    .filter(lambda w: len(w) >= 2)                    # remove tokens with length < 2\n",
    "    .map(lambda w: (w, 1))                            # prepare for counting (word, 1) pairs\n",
    "    .reduceByKey(lambda a, b: a + b)                  # count occurrences by key\n",
    "    .sortBy(lambda kv: (-kv[1], kv[0]))               # sort by count desc, then word asc\n",
    "    .collect()                                        # collect results to driver\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d215c",
   "metadata": {},
   "source": [
    "## 3. Word Count with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5672f0-63ec-4f0e-b784-1684e1c7a7e8",
   "metadata": {},
   "source": [
    "### 3.1 Again, Just with DataFrames\n",
    "\n",
    "Now, we're going to do the same thing, but with DataFrames instead of RDDs.\n",
    "\n",
    "What's the difference, you ask? We'll cover it in lecture soon enough!\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Here, you'll use the `SparkSession`.\n",
    "- Loading a DataFrame is a single method call. If you find yourself writing more code, you're doing something wrong...\n",
    "- When loading the CSV file, be aware of your escape character; use something like `.option(\"escape\", ...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e99b288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 7261\n",
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------------------------------------+\n",
      "|                                                                     description|\n",
      "+--------------------------------------------------------------------------------+\n",
      "|a-case is a brand specializing in protective accessories for electronic devic...|\n",
      "|A-Derma is a French dermatological skincare brand specializing in products fo...|\n",
      "| a patented ingredient derived from oat plants cultivated under organic farmi...|\n",
      "|                                                                       cleansers|\n",
      "|           A-Derma emphasizes clinical efficacy and hypoallergenic formulations.|\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "======================================\n",
      "Wall time: 3.515 s\n",
      "RSS Δ: +0.12 MB\n",
      "Peak memory Δ: +0.12 MB (OS-dependent)\n",
      "======================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExecutionResult object at 7298c36ee980, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7298c36ef340, raw_cell=\"\n",
       "# TODO: Write your code below, but do not remove ..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "# Unlike RDDs which are Spark's low-level API offering distributed collections with basic operations,\n",
    "# DataFrames are a higher-level abstraction built on top of RDDs.\n",
    "# DataFrames provide a schema (column names and types) and enable SQL-like operations.\n",
    "\n",
    "# Load the CSV file into a DataFrame using SparkSession\n",
    "df = (spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")  # Assuming the CSV has a header row\n",
    "      .option(\"escape\", \"\\\"\")    # Setting double-quote as escape character\n",
    "      .option(\"inferSchema\", \"true\")  # Automatically infer column types\n",
    "      .csv(\"a1-brand.csv\")\n",
    "     )\n",
    "\n",
    "\n",
    "# By the time we get to here, the file should have already been loaded into a DataFrame.\n",
    "# Here, we just inspect it.\n",
    "\n",
    "print(\"Rows:\", df.count())\n",
    "df.printSchema()\n",
    "df.select(\"description\").show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882eacc",
   "metadata": {},
   "source": [
    "Next, clean and tokenize text, and then find the 10 most common (i.e., frequently occurring) words.\n",
    "This attempts the same processing as word count with RDDs above, except here you're using a DataFrame.\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Required Steps:** (Exactly the same as above.)\n",
    "\n",
    "- Lowercase all text.\n",
    "- Replace non-letter characters (`[^a-z]`) with spaces.\n",
    "- Split on whitespace into tokens.\n",
    "- Remove tokens with length < 2.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You _must_ use `explode` and other Spark DataFrame operations in this exercise.\n",
    "- This exercise shouldn't take more than (roughly) a dozen lines. If you find yourself writing more code, you're doing something wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec84df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "\n",
    "# By the time we get to here \"word_counts\" is a DataFrame that already has the word counts sorted in descending order.\n",
    "# So we just print out the top-10.\n",
    "\n",
    "top10 = word_counts.limit(10)\n",
    "top10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede1ca2",
   "metadata": {},
   "source": [
    "**Questions to reflect on**:\n",
    "\n",
    "- What is conceptually different about how Spark executes `flatMap` and `explode`?\n",
    "- What are the advantages or disadvantages of using each of them? \n",
    "- Are there cases where you may prefer one over the other?\n",
    "\n",
    "(No need to write answers in the assignment submission. Just think about it...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505e64e",
   "metadata": {},
   "source": [
    "**Question to actually answer**:\n",
    "\n",
    "Does the RDD approach and the DataFrame approach give the same answers? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a1265-558b-464f-8c85-e9c553ba9fad",
   "metadata": {},
   "source": [
    "**Write your answer to the above question!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ca759",
   "metadata": {},
   "source": [
    "### 3.1 Removing Stopwords\n",
    "\n",
    "You've probably noticed that many of the most frequently occurring words are not providing us any indication about the content because they are words like \"in\", \"the\", \"for\", etc.\n",
    "These are called stopwords.\n",
    "\n",
    "Let's remove stopwords and count again!\n",
    "\n",
    "**write some code here**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Filter out all stopwords from the DataFrame before counting.\n",
    "- Use `StopWordsRemover` from `pyspark.ml.feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "import numpy\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "\n",
    "\n",
    "# By the time we get to here \"word_counts_noStopWords\" is a DataFrame that already has the word counts sorted in descending order.\n",
    "# So we just print out the top-10.\n",
    "\n",
    "top10_noStopWords = word_counts_noStopWords.limit(10)\n",
    "top10_noStopWords.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623f34a",
   "metadata": {},
   "source": [
    "### 3.2 Saving Results to CSV\n",
    "\n",
    "+ Save the results of the top-10 most frequently occurring words _with stopwords_, as a CSV file, to `top10_words.csv`.\n",
    "+ Save the results of the top-10 frequently occurring words _discarding stopwords_, as a CSV file, to `top10_noStopWords.csv`.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e082af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89fac7f",
   "metadata": {},
   "source": [
    "## 4. Assignment Submission and Cleanup\n",
    "\n",
    "Details about the Submission of this assignment are outlined in the helper. Please read carefully the instructions.\n",
    "\n",
    "Finally, clean up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b071ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96409bb6",
   "metadata": {},
   "source": [
    "## Performance notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c5299",
   "metadata": {},
   "source": [
    "- Prefer DataFrame built-ins; avoid Python UDFs for tokenization where possible.\n",
    "- Keep shuffle partitions modest on local runs.\n",
    "- Cache wisely and avoid unnecessary actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f18f3",
   "metadata": {},
   "source": [
    "## Reproducibility checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64031975",
   "metadata": {},
   "source": [
    "- Record Python/Java/Spark versions.\n",
    "- Fix timezone to UTC.\n",
    "- Provide exact run command and paths to input/output files.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de1-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
