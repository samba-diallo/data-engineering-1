{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DE1 — Lab 0: Installation and Sanity Checks\n",
        "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
        "---\n",
        "\n",
        "Goal: **prove** your local setup using metrics and execution plans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Preamble\n",
        "- Activate the `de1-env` environment.\n",
        "- Verify Java 21 and Spark 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
            "Platform: Linux-6.11.0-19-generic-x86_64-with-glibc2.39\n",
            "openjdk version \"11.0.1\" 2018-10-16 LTS\n"
          ]
        }
      ],
      "source": [
        "import sys, os, subprocess, json, datetime, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "# Java version\n",
        "try:\n",
        "    out = subprocess.check_output([\"java\",\"-version\"], stderr=subprocess.STDOUT).decode()\n",
        "    print(out.splitlines()[0])\n",
        "except Exception as e:\n",
        "    print(\"java -version failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Verify PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PySpark: 4.0.1\n",
            "Spark: 4.0.1\n"
          ]
        }
      ],
      "source": [
        "import findspark, pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "findspark.init()\n",
        "print(\"PySpark:\", pyspark.__version__)\n",
        "spark = SparkSession.builder.appName(\"de1-lab0\").getOrCreate()\n",
        "print(\"Spark:\", spark.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate a tiny local CSV and read it with Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote data/sample_sales.csv, bytes: 154\n"
          ]
        }
      ],
      "source": [
        "import os, csv, pathlib\n",
        "pathlib.Path(\"data\").mkdir(exist_ok=True)\n",
        "rows = [\n",
        "    {\"user_id\":1,\"product_id\":101,\"price\":9.9,\"ts\":\"2025-09-01T09:00:00\"},\n",
        "    {\"user_id\":1,\"product_id\":102,\"price\":19.0,\"ts\":\"2025-09-01T09:02:00\"},\n",
        "    {\"user_id\":2,\"product_id\":101,\"price\":9.9,\"ts\":\"2025-09-02T10:00:00\"},\n",
        "    {\"user_id\":3,\"product_id\":103,\"price\":5.5,\"ts\":\"2025-09-03T11:30:00\"},\n",
        "]\n",
        "with open(\"data/sample_sales.csv\",\"w\",newline=\"\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=[\"user_id\",\"product_id\",\"price\",\"ts\"])\n",
        "    w.writeheader(); w.writerows(rows)\n",
        "print(\"Wrote data/sample_sales.csv, bytes:\", os.path.getsize(\"data/sample_sales.csv\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            "\n",
            "+-------+----------+-----+-------------------+\n",
            "|user_id|product_id|price|                 ts|\n",
            "+-------+----------+-----+-------------------+\n",
            "|      1|       101|  9.9|2025-09-01 09:00:00|\n",
            "|      1|       102| 19.0|2025-09-01 09:02:00|\n",
            "|      2|       101|  9.9|2025-09-02 10:00:00|\n",
            "|      3|       103|  5.5|2025-09-03 11:30:00|\n",
            "+-------+----------+-----+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"data/sample_sales.csv\")\n",
        "df.printSchema()\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Execution plan — evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+-----+\n",
            "|user_id|  n|total|\n",
            "+-------+---+-----+\n",
            "|      1|  2| 28.9|\n",
            "|      3|  1|  5.5|\n",
            "|      2|  1|  9.9|\n",
            "+-------+---+-----+\n",
            "\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[user_id#17], functions=[count(1), sum(price#19)], output=[user_id#17, n#39L, total#40])\n",
            "   +- Exchange hashpartitioning(user_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=84]\n",
            "      +- HashAggregate(keys=[user_id#17], functions=[partial_count(1), partial_sum(price#19)], output=[user_id#17, count#52L, sum#53])\n",
            "         +- FileScan csv [user_id#17,price#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/sable/Documents/data engineering1/data/sample_sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:int,price:double>\n",
            "\n",
            "\n",
            "=== explain formatted ===\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan (5)\n",
            "+- HashAggregate (4)\n",
            "   +- Exchange (3)\n",
            "      +- HashAggregate (2)\n",
            "         +- Scan csv  (1)\n",
            "\n",
            "\n",
            "(1) Scan csv \n",
            "Output [2]: [user_id#17, price#19]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/home/sable/Documents/data engineering1/data/sample_sales.csv]\n",
            "ReadSchema: struct<user_id:int,price:double>\n",
            "\n",
            "(2) HashAggregate\n",
            "Input [2]: [user_id#17, price#19]\n",
            "Keys [1]: [user_id#17]\n",
            "Functions [2]: [partial_count(1), partial_sum(price#19)]\n",
            "Aggregate Attributes [2]: [count#50L, sum#51]\n",
            "Results [3]: [user_id#17, count#52L, sum#53]\n",
            "\n",
            "(3) Exchange\n",
            "Input [3]: [user_id#17, count#52L, sum#53]\n",
            "Arguments: hashpartitioning(user_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=84]\n",
            "\n",
            "(4) HashAggregate\n",
            "Input [3]: [user_id#17, count#52L, sum#53]\n",
            "Keys [1]: [user_id#17]\n",
            "Functions [2]: [count(1), sum(price#19)]\n",
            "Aggregate Attributes [2]: [count(1)#45L, sum(price#19)#46]\n",
            "Results [3]: [user_id#17, count(1)#45L AS n#39L, sum(price#19)#46 AS total#40]\n",
            "\n",
            "(5) AdaptiveSparkPlan\n",
            "Output [3]: [user_id#17, n#39L, total#40]\n",
            "Arguments: isFinalPlan=false\n",
            "\n",
            "\n",
            "Saved proof/plan_formatted.txt\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "agg = df.groupBy(\"user_id\").agg(F.count(\"*\").alias(\"n\"), F.sum(\"price\").alias(\"total\"))\n",
        "agg.show()\n",
        "plan = agg._jdf.queryExecution().executedPlan().toString()\n",
        "print(plan)\n",
        "print(\"\\n=== explain formatted ===\")\n",
        "agg.explain(\"formatted\")\n",
        "\n",
        "# Save evidence\n",
        "import pathlib, datetime\n",
        "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
        "with open(\"proof/plan_formatted.txt\",\"w\") as f:\n",
        "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
        "    f.write(str(plan))\n",
        "print(\"Saved proof/plan_formatted.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Spark UI — metrics to log\n",
        "Open `http://localhost:4040` while a job is running and record:\n",
        "- **Files Read**\n",
        "- **Input Size**\n",
        "- **Shuffle Read**\n",
        "- **Shuffle Write**\n",
        "\n",
        "Fill the provided `metrics_log_template_en.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "de1-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
